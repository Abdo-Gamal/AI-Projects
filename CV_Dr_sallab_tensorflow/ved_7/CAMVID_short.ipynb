{"cells":[{"cell_type":"markdown","metadata":{"id":"X6htWXtLC9wz"},"source":["# Semantic Segmentation\n","\n","\n","## Class Heatmap\n","![Heatmap](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSRd6rCHDMj1Yf8lrDD5in14daoDyegD-EfCI6fwU7dSD9VB9OW)\n","\n","## Different CV Tasks\n","![CVTasks](https://miro.medium.com/max/2000/1*cHtBw8yBhprNXj-CBQBx5g.png)\n","\n","## Traditional way (CRF)\n","\n","With the convention of __super pixel__ it is clear that we can represent a segmented image in a much lower dimensional representation.\n","![super_pixel](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS13gXULV9qrPK7hcC8Lz-UONCl81z28vL1Y_AvD2ImDSifcDTcIA)\n","\n","If we have this super, we can plug a softmax on top of the pixles inside the super pixel to get its class label.\n","\n","__But how to define a super pixel?__\n","\n","With convolution filters spanning the image, they can be a form of a super pixel.\n","\n","But this would be a shallow model. No spatial hierarchy.\n","\n","A better idea is to use a deep conv model, and use the final feature map (say 7x7 or 4x4) that divide the input into large grids, which are very good super pixels.\n","\n","Having done this downsampling, we are left with 2 more operations:\n","\n","1- Upsampling again to the original image size:\n","done with deconv or bilinear upsampling\n","\n","2- Class heatmaps:\n","done with 1x1 conv with output feature maps = n_classes\n","\n","We will see that in detail next:\n","\n","## Fully Convolutional NN\n","\n","![FCN](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAkGBxQSEhUSExIWFhUWFxgbFRYXGBgZGRgXHRgbGB4YGhUdHigiHR8nIBkYIjEiJSkrMC4uHx8zODMsNykuLiwBCgoKDg0OGhAQGzIlICU3Li41LSstLS0wLSs3LSsrLi0tLS0tKy0tNS8vLS0tLS01LS0tKy4tLS0vKystLS0tLf/AABEIAKABOwMBIgACEQEDEQH/xAAcAAABBAMBAAAAAAAAAAAAAAAABAUGBwIDCAH/xABMEAACAQIDAwYICQoEBgMAAAABAgMAEQQSIQUxQQYTIjJRYQcUNEJxc4GyI1JTYnKRlKHTFjM1VIOxs8HR1CSjtPAVY4KSwuFDovH/xAAbAQEAAgMBAQAAAAAAAAAAAAAAAgMBBAUGB//EADIRAAICAQIEAwYEBwAAAAAAAAABAgMRBCEFMUFRErHwE2FxkaHRFCKBwQYVIzIzNOH/2gAMAwEAAhEDEQA/ALxooooAooooAooooAooqAcvfCXFggYsOBNiN3/LjOvXI6xHxB7StZSyCY7X2tFhU5yZwo4Dix7FXiaVYaYOiuNAyhhfvF6oyeOXEuZ8S5ZjrruUDgBuA7hpV3bN/Mx/QT3RUVnO4FNFFFZAUUUUAVrw+IWRcyMGU7ipBH1ispRcEdx3G338PTUaXZGKWDm0lswTKvwjaDxbmwt/XfCZ7Xtp3UBJ6KZMVsqUyApK4jsoZTLJfz1azXPmuCD8aNe0mtbbPxJLgv0WdCMsrgqomdmsd+sTKtgRqNANDQD5zq5smYZrXy3F7Xte3ZcjWvIp1YkKwJG8A6gXIBt2GxseNqZ/FpkfnCcxyzIGzE9KSVOb6FrBQBr6OO+tk2AkEihCwhURDKJGU2VZVbQfSi465SeAuA6eMLn5vMue18txmy6C9t9tRr3itlRuHZWItdnIZsnOFZGGZhAIyRpYWcX3ajXhY7/+GznXnXF2muBIeLkxMCQQMqkgra2o35QCA+0UxQ4HEA6ucua5XnGuVzykKG3r0Wi3b8hB33pVs3Byo5MkhYWbTMSLEjLoRvUArfS+8gk9EBzoopHtXasOGQSTyCNCwXO2igndmbco4XNhew4igFlFa4J1dQ6MrKRcMpBBHaCNDWygCiiigCisJZAouxsBUXmx0kpbmmZ1fnBGyEZrDoyR7gqm2WSKQ79ASARmAdNobYyGQqRaIWlDKRkLKGSQvcDmwM2awOlzcZSCiOCeX4WWRo0KrcPlzruYAFTkDpIFKyLcG7CzdFgsgwixlbqjzAHSNebQBr3LrmIALF21udWy3N772NmF/hJd6qNFS+gPHKN/SN2PStfq0BtbFMFzZAqcDI5U9xIym1+w69oBowOMZywaPJYKRre6texIIBXqnQ67r2OlZxYXUPIczDd8VOHRXttfpHXU7hpSTEY5IpHZmUFhEqKWUZrvkDDUnLmlVb239txQDpTNNjTiIy0DMpV1yGxyyXRXAJCm0bK9iw6t76MLUhO2ZELYidligQtnMlkQJmZLXuSZVeM2t0XVxxIC0ny88KPOyyLs4NEjFs05AErXFjzZAvGh6TWvclieiTagLuTljB49Hs/OHxEmYsiWIhCozfCPe2Y5eqLkX7LEyWuX/ATC7bXicKxCrKXYAkLeNgCx4XOmtdQUAUUUUAUUUUAUl2ltGLDxmWaRURd7MfuA3kngBqaYOVPLnD4NuZzCTEWuIlPVGmrt5o1Gm89ltapLlHtzEYvEO88hYAjm0GiICAbIvDfvOp4nSpxg2CX8rfCLLibxYbNFDqC26SQekdRe4a9p1Iqu8YoutyAuZbk7gMwBP31li8ckQux14KN5/p6ajmNxzSm50HBRuH9ask1FYRmMWyUcr+WhlzRYa6xm4aS1mf6I81fvPdXSuwfJoPUx+4K43bca7I2D5NB6mP3BWuTmsC+iiislYUj2rs5MRHzbmQKSDeOR420+ehB++llNHKbE4uOEtgsPHPJr0ZJOb9FhazegsvpoBB+RGH+Vxn23F/i0fkRh/lcZ9txf4tQbkdyi23JjpkxUC5xHeOCRzhowuZQzRkRvztjlFyTbNv1qdeP7U/UcL9sf+3oDz8iMP8rjPtuL/Fo/IjD/ACuM+24v8WvfH9qfqOF+2P8A29Hj+1P1HC/bH/t6A8/IjD/K4z7bi/xaPyIw/wArjPtuL/Fr3x/an6jhftj/ANvR4/tT9Rwv2x/7egPPyIw/yuM+24v8Wj8iMP8AK4z7bi/xaBtHaZ1GBwv2x/7evfH9qfqOF+2P/b0B5+RGH+Vxn23F/i0fkRh/lcZ9txf4te+P7U/UcL9sf+3o8f2p+o4X7Y/9vQHn5EYf5XGfbcX+LTPys8GqYnDmCKedS7LmaXE4mVVUMGJ5lpCrnQAA2te97gU8+P7U/UcL9sf+3o8f2p+o4X7Y/wDb0Ag5D+DfD7M6Uc07udWJkZUJtb8yhCkfTzW+qppTPsnFY1ntiMNBEljZo8Q0rZriwymJNN+t6dDOtyMy3UAsLi4BvYkcAbH6jQGyk+LnKo7IvOMo6gIBJtfLc6A27aaMVtV2F4blHOVHFm6WUNYINbHpC7MLMtrC+Ybdm4cwXaRs8khY5VUZyC5e17noqXNrmy5rFmJzEBHDC+JZpHcGA2yurMgAXJKsiIQbHMWR1csCE4AshdY9QShyL50zAZmAvqLi1t/SItbcLEEaxAsfWUFmZnWGPUFi2Ysb2zHMb5msoOXcdSqXDFyGlsbWIQdQEa3PxjfidBYWAIuQNUALC0QyJxkOrP3rm33+O1720BBvSuGFY1sBYbySd/aWY6k95rVj9oJCLudbMQqgs7BRmOVBq1h2CmLE4l8QXDHm4UUFnV0yq1i+cuw6uUxSKy31BDBbHMArxe2WIdYQMysljYyBkkU5JURNXTNv3aJJbcCYdyo5W4XZkYTEDPMLvFg1NyM4YFJJuMZ10Ybioym1Q/lp4WVizQbOs0l25zFlbKGaxfxeMk5QzC5Jvrc66NVPySPNISxaSR21JJZ3Ynt3kk0A+csOWWJ2k+ad7Rr+bhTSOMbtF4n5xufZpWzkzySfEATTExYfg1unJbeIlO/sL7h3nQvfJ/kasNpMWoeTzcPfor3zEbz8wH6XxaksshY3J7hwAA0AAGgA3ADQVzNXxCNf5a935HI13FI05hXvL6IkHg5jSPFQwwoI4hzhCAk3bI3Sdjqzd59AsNKt2qi8H/l0Xof3Gq3anw2cp1OUnl5/ZE+E2SsplKTy8vyQUUUwcqOV2GwIAle8rj4OFdXbvt5q/OOnZc6V0DqD1i8SkSNJI6oii7MxAVR2knQVSfLvwwtKThtmnIp6JxLaMbm3wakdEfOOvYBoajvhA21i9pSKC1o1UskCtZcwa17ecwBHSbvta9qr9B0spIGtieA39m+pOLXMDps/FCPEFmLNcHM2puc3Sc7yb5bk6k0p2vtRQ7CMhjprwHRH17qbp8WqgpDcKRZnPWfu+avd9dIay54WEWRr6sydixJJuTvJry1e0p2dhudlRPjML+jjVZdjAkkUi9xbT+V67G2D5NB6mP3BXJfKZbYmUdmUf5a11psHyaD1MfuChVY8pMX0VrnmVBdjbs4knsAGpPcK0c00nX6KfEvqfpsOHzR7SQbVkqMvGwSQis9tCVy2v2XJF/Ze26sZcaVGZo2AG8kx29+spcQFsiLmawsg0AG4Fj5q/wBDYG1qIsLqHkOZhu+KnDor22v0jrqdwNqA8gmE0RbIQDnGVwAdCVII9hplj8bSJFVW6KLa/NXzKmqEbipNgCCNza2y3etm9Q+sl/ivSqgI5j48QwVghaRZXKA83lQZJlVlbU3Ksg1vqfSK3KuK51QGfmiEJLCEMt1FwQBvBja9tCZhbRdH2igGCF8WxUMHQFwWIERKqUkuu7g4j1A4nUjWiATK7u6kSPFGmayZRJncWW3SKjOGF+F+NxT/AF4RQDA2FmjeQQLkUtfMFVibQRomhIut1IOoOi6gXNehsWzFTnRS4swETFQTKCN24Dmju7dTT/RQEfPjDaMpYc6pGZY7BVxWh9PNANfeCBax0rTLjMUgGYnXKAWEYN/hri4FhosJ1BvcganSTUUBowT5o0bMWuqnMRlJuL3K2FvRW+itWJRmRgjZGIIVrBsp4HKd9uygEuP2qkSszBuje4ytbqlrlrWVdCM5IW+l76U1CJsUxeyFQxUXAeN4s+vTFw11yMANVdNSAbVjgcMGLSymRDdo2jzMxY2syB7ZnjJGdSOkNdVBZA6zbgZeih0WIalidbNbrG3mrp1r5huAwwMCqCuHGhPTmYlrn6R1kYbsxPpJItWcBvcQ6368za3Pd8Y8NLKtz2Za2CBpPzgypwiHEf8AMI0PZlGm+5bgsAoDVh8MEva5J6zHVmPefr0Gg4ACm7au2ljbmk1l6OgGYqCetzYIZhx04Bt5W1O5qofCL4X4cOxiwOWaddDNe8MfbYA2kca2O4X3nUUBIOUG148GTJinPSZXRVzc82UuRzeo5sC+QnihAIW5JrXlXyvkkjvMVw8HNt4tg4975lZVdl06NzfO1hp0QTekm3sTkxgzSc/iJ/hBMxDxQQksQ9j+cZUUtY2RbDRuCOfADahWaSQRiJmSQ6vKYjZolHx3/OguxG65JNgb5ONdcprkubfTPb7lLzhzm8RXrchOxtky4qQRRLc72J0VF4s7HRR/6AuSBVk7C2NFgh8HaSbzpyLEX0yxA9UfO6x+aNKWYaJIoxDCmSMcL3Zj8eRrDM33DcABpXteZ1fEJWflr2Xmef13FZWZhTsu/V/YK2QQlzYW9JNgPafq9JArXTTtSb/E4KK/WnjZh3c4FH/l9VaVFXtZqJo6HSvU3KvpzfwROPB/5dF6H9xqt2qi8H/l0Xof3Gq3a7XCv8L+P7I7vBf9d/F+SCuVtvSk7VxbMxJ8axIuTc2WR1A9AAAHcBXVNclcvBl2hiyp0bE4j6+ecMPr/fXTTw8nXSyI22jnEPOFgojYNlPSk1AIvwuV19BprxDg6gBQzEhRwG4D2bvZWpuAHoH3/wBa9GuVbdIaffp++suTZLGDaBXoFelbb68ZgN9VmyO0OyM+G54HUE37LDSlWw4eZdZG7/qrPk9tEeKSxnfnNhxsQv8A7pVhMDnXO7hQFJUHe1iFsB7b+ys7IolJvYje15C0sjNvYk+y2n3V1nsbE/4eBEGZhFFf4q9BT0j2280a6jcDeqH8F/I6PaOKmafpJAIyY7lRIXzAXYahRkNwN9xqLEHoPDwtGoRI4lVRZVUkADsAC6VkxKWUjOOALd3a7WN3OgA3kKPNX+guTa9Yc60nU6KfKEat9BTw+cfYCDca8Ihm6cmtncKg6oyOVzH4xut9dBpYXFy4UIGuCBUFlFu3eST2knUnvOtbKTzYmxyqMz/FG4dhZvNH3nWwNqxXCk6u7E/NZkUdwUH7zc+ywABs3qH1kv8AFelVNSxB9IcwW+smd7am5yDN0j87dc+dqK37IjyoVzM1nk1dmduuT1mJPs3AaCwAFALqKKKAKKKKAKKKKAKKKKAKKKKAbzh1fEOWUErFHa/C7yXt2XyrftsOyvMNCq4mSwAtFFbuu8t7enKv1DsrbF5RJ6qL3paxh8pl9VD781ALqKKKAhPhpkK7GxZUkG0QuDbQzxqR6CCQe4muVEW5AHE2rqjw2/oXF/sf9RFVXbA5PxYAg6NOovJM3Vj0ueaU7gB/8h1I1GW9U3XRqWX15Luyi/UQpjmXXZLuxt23hkUPis93+GgRAOrmnmLMzd6FlAA48NAdfI3dMPVk/wD2H86Y58e7pkJ6Odnt85v5b7DvPbT3yK3zC4uVWy3F2s2YkDjYDhw13V1tdp3DQXJ7t5e3xWC3XVS/B2p+/l9CRV6tr63txtvt3V5akO0dsxwXF88in82A3RI+OSLW7gSeGnDwul0tuon4Ko5frmeJ02msvmo1xz66jZ+WyGME4Yc9c3AYiEjgco6d+Fg3C9+FNOzdrZ8dFPPa3OKSB0VQX6NhwVTY+zUk6017RTLLINNHYabusd1J670aYQb8KwfRqtFRXl1xS8XYvbkEpGPjB3gSA/8AY1W5VDeCTlQ0+PhiljUyZZLSp0b2jYnOm4nvGXvvV800NLqrcX3+xxtHo56SDrn3z+mwVyby0UeM49j+vYgAcfzrk2FdZVybyjjDbWnUi+baEoPePGWvW5g3U8Edx2HySc3xWwf6e9h7Ccvsr3m2Z7DW9yB6Bf8Aka3y2ZnkzXYuWvwsSSTf08KV7Ew+fGQJbQt91rn7gaGB32/sUvh0xIFmCKZB2i2p9PGoXJV6vhQY8ttN1u61v51SO0YgkjoPNdl+piKyzEXnYe+R+XN0xcXuR22sLfvqfDk87wxOL3K2ddwuWZgbegDTtIqO+DzZImygjSxPt1q1Ga7EDQXvl7Dly3PfYGspdTEmafA9gRDiMWnHmcOW7znn1q0qrPwWzB8ftEjcFw6j0KZh++9WZWGEI9k9Q+tm/jPWOPlkzpHHYZldmYmxAUoLL0SLnPvPZ33GWyeofWzfxnryXyiP1UvvQ1gyewI6Cyxxgesa5PaSUuT3nU1qgUzgmS2UMy82DdTlYrdjYZt3VsBrrfQhxpHsvqt62X+I1AZyYkklYxcjex6q+k8T80d1ytwa1rAY1YmWwuzMSFAFzc68BWjY+CjMEJMaEmNCTlGpKgk7qyx+EjAQiNAedj1CgeeO6gNIxUzuBEQVB6ZkW2lu6xBGhsdTxCghqXsjAEtLYAXJsoA7TruFJtl4KMwxkxoSVBJyjUkXJOm8nWjaOEjCqRGgIlhsQoB/PJ3UBoOLmZwISGAYZi62GXS+7UaG4vqdNMpz04FGAu0trC5OUADt37h7aT7PwcZS5jQks5JyjUl2JO6sdp4OMIpEaA87DqFHyyd1Aa5sVL0WRuhnjBLL1w0iocgFiB0icx7BYEG9O9NuNiaWTmwwCqqORYm7FmINwwIsUB0r3xB/lB/m/i0A40VGsDG0srWna65w63kKdGUqoAz9EjJc2Nzm10sKdPEH+UH+b+LQDjRUZwSPJLpO1xz6uoMhToShFFs/RNhc636XxSBS3aEEkcUkgkBKIzAHnbaKT8r3UAti8ok9VF70tYw+Uy+qh9+akWwktNPZ3dSIirOxa4s1yCfNzZt1hvtS2HymX1UPvzUAuooooCDeG39C4v8AY/6iKqafHsNmYcsbvJGY17ciyOPbZFRPQwq5fDb+hcX+x/1EVc/46U83h4ibiOBLftPhvuEir/01dptOrro55Rfi+XL64CpVso56PPy/6I69RyCCCQQbgjQg9oNeUV6Q6Ju8bkvfnHvvvmN79t61ak9pP3mvKW7GcLKHIVggLENexsNBoQb3It32qEmoRckuRHCS2GrFXzvffma/pvWqiivIt5eTZROfAr+l4Poy/wAJ66ZrmbwK/peD6Mv8J66ZqyHI0NV/eFcl8qWybTxTWv8A4zEH2c+4P3XrrSuS+WCn/iOKI1/xeKFu/nn/AP366ma6GKFdLDdm393AVK/B3gecxTyMPzS6aee5y/uDUzQbMaOMyONScsaDUliNDb9w9PZVj8hNkth4RnFnc53B3rcWVfq1PeTRiSwtx5xziOMsTYC5Po/2BVD7QkzySN8Z2I9rE1bHhNxnNYYL50jAD0Ahj+63tqpI48zKo4kCssjBdS8PBhs0Lg0e2ri/s3D+tSGTD5XLg2UK1x2k2sfZY/XTH4PNrGSBoimVYgqoRuItYfurLlZttY4mXN0iCABvJ/3xrJhJt4FPgRUmfHSEWDiEL7Glv7376tqql8BWLLnEofMWKx3aFpT/ACNW1USyUXF4Yj2T1D62b+M9eS+UR+ql96Gvdk9Q+tm/jPXkvlEfqpfehoRFtI9l9VvWy/xGpZTRhscEVwLFhJLfWwW8rWzNwvcWGpN9AaAzwGMWKCMOHUqiKfg361gLCy6m+mm+stoYjMFypIxEiG2RhuYHewAHtIpn2jjpFNjm503KOVy9DpdGNSHCNbpEMGZlSTQWFlew8NMZWxLEKsujxWa5ZFVFkBJ0ByuQMoJVkuRltQC7ZuIyxRqyuGCKGGRzYgai4XX015tHEZkAVJGPORG2RxukUnUgAaA7yKQbd2tKvwcamNidGNjmXpDonKwBFg5BBPNrIbAis9i4WYzPiSQiTDpxWa5dVRFkBNrCytplBIKXsRloBdgcRlQBlcNdrjI5t0id4Wx9IrHaOIzIAqyE85Ebc240Eqk6kAbgd9J8fj5S2RAI8hzu0hADRAspswDBLkKekDdc3VOq6eT2FnDmV+ikudyh62ZyrAEFAej01BJuVy3AsAoCzGRobSYnIFB6KGxAO8EnziLXsNB32DUx4zays4jhhS4kOYhQx5pWyEsoRstyY2A3tGSwIsSrli9kzPiecWQKl1O9iwKr0SqEZV1LhrddZGBtlWnLA7PWILbUquUHsUm5VRuUbtBwVRrYUBoE8QhjknRIrqDlcDoErmK6gbrEnQaAkgWNmLFbVVpBHFAt1c57KrWjBZTnUIWHS5u6gE5ZEYEDMVcZ9kzNiWlEirHmVl1ZmDKgUEIeio1lVhqWV96lQadMBs9IlQAXKIEDHgo4AbgNBoOwdgoDQ4Q4YSPHzdo8xUIHeO65mCplN2HYFNyNxps2YLtBzir8Ist15uPg45vOVWwfLo1jluGtfQ1JaKARxeUSeqi96WsYfKZfVQ+/NWUXlEnqovelrGHymX1UPvzUAuooooCDeG39C4v9j/qIq5+2quWTId6JEjfSSJEYexlIroHw2/oXF/sf9RFVA7eS2JnB+Vf2gsSD7RY10eGr+o37i+jmxDRRRXZNoKctg4cO0hJAyQyN0t3VI/n9dqbaWxzZMLN2yMiD0DMx146aHsuta2sn4aJMw+wy0UVNdh8lFQZ8WhLMOjBdlKg+dIRqrW3L33bdY+TssjXHxSZnU6qrTQ8djwjZ4Ff0vB9GX+E9dM1zn4MMGIdvpEL5VM4W+pK805Uk9tiK6MrYr3Rq6hqUk12CuVOVNk2hjC3HE4kgAXP55xp7Ca6rrljlfHnx2KAGpxc49nPSD6tBUmQr2HrkmomJmWy5LhAdewm54nQf7vUt2fPc2O8m9NWBw6wRIF0BUg+k8frP31tguzdHhpm778BxqWCqUm3lkU8K8rNMi+asYYD0sVJ/dUN2ZhTfOeG4fdU45e3E6AbzCFJ42zk27qYtmqvOx5iAudcx4WB1o+ZdCtuHiLMw+DfD4ciFbtkF+9rVW2NxLOxZiSTvJq5MLKsg6JBBF7g6VUu2MC3jckMaMzGRgiKCWN9QAo1O+kyeje7RYPgA/OY36GH/AHzVclV34J+SU2AMrTlQ86RnmxrkCl9GYaEnPuGgtvNWJUUV3NObaEeyeofWzfxno2lLCgDTZANbFgDwubewEnsAJOgrHZjARsSQAJJySdwHPPqTWOMRsRHJGvQV0dRIQb3ZSuZU0NhfeSL200N6yVjJjcaCVWPD5CJWDAwkkxLdC9hGcy5yukZJysrXXWz5s3DgqkjJlYqGEZtaIsNVAAAuLkFrXOvDQbsPg1UhiSzhcuY2uFvewAsFGgvYC9lvewpTQGl8MpdXIuy3y3JsLgi4W9r2JGa17Ejca3UUUBpfDKXDkXZercmw3i4W9gbEjNa9iRurdRRQCY4GMy88VBkyhQxubAZrWG4HpvqNbEilNFFAFFFFAFFFFAFFFFAI4vKJPVRe9LWMPlMvqoffmrKLyiT1UXvS1jD5TL6qH35qAXUUUUBBvDb+hcX+x/1EVUTtnCzZMNKyEo0ESxyaHPaNSQSN5XNl11soHCr28Nv6Fxf7H/URVQfJ6ebCYeR+cZfGUKpEGIBQ3DTMt7dqJftc6WF9rRzlG1eFZzsWVNqWw30V5XtegN0U7OwEk8gjiXMx17AAN7Mx0VRxJ0r3lFEImTDq4fmxd2ANjI9i2W+pAUIu4agnjU12RCIYFRRlzqjyHznJAYBj2LfRd3HfrUf5VcnJM74mH4VHJdwo6cRJJIZN5UfHAt22rymq4stRdKiO0V9Wc3S8Sru1Uqs4x36vrj1uZcg44vhJCLzx5THmtZVOhkUfHDZRc7swI11ElJvqd9QHkviubxKHg10P/UCo+8g+yrO2NsWbFPkiW9usx0VfS38t9cDXwlK1Jb56HE/iGqyWqill5Wy88CfwebNkO3FnKkRiNiHIOVjzOTKG3Zt5tvsDV70w8nOS8WEGYdOW2shG7uUeaPv76fq7OnjONaU+Z16XY64qxYaSXyCuaduwhcbii5tmxeI38FE76+3+ldLVy9y6xIbG4hQNVxM9z386wtVrNqpZY4TbQbGP4tDcRnrHziBrp2VKMTi48JCHbfayrxJqMckSMPE2IbTQkejcB9dR3G455Wu7E9l+A7BUk8bk1SpywuSNm1NoNO5dvQB2C5NvvNJFoUXIA1JNgBqSTuAHE1aHIjwUvJlmx4KJvWAEh29Yw1QfNHS7Su6oczbbjXHcR+CPAzTSyZVbmstjIb5AwPVB4tYnQd17Vb+yOTsGHd5UjBme2eUjpEAWsD5o0Gg9t6cMJhUiRY40VEUWVVAAA7ABW6p9Dn2T8UnJCLHHmyZswAC2IKljv0sAbk62tbXSkuH2uHcRrIhcgnLka4ysyG/S0syOPSKcsVAJEZDuYEHQHf3MCD7QRTXg+TcMTq6ZgV1I6FnbNK+ZrLoc08rdGw6R0tpQgKsJs/L+cbP03ZQBlUFnL3y3NyCd5PAEAa0voooAooooAooooAooooAooooAooooAooooAooooBHF5RJ6qL3paxh8pl9VD781ZReUSeqi96WsYfKZfVQ+/NQC6iiigIj4WMMsmysQkj5EJhLtxyieNiF7WIBAHEkbq5/g2fJi3aVVCR5guY9VFAAWNeLZVCiw1ta9t9dBeFXCvLsvERxqWY81YDunjJ+4E1U+HhWGMJfoxqSzDjbpM37/ZYVmzXfhKs1rNknhL3d/ma2r1v4ateBZnJ4S/cjO2ZVgU4SICxytK5ALO1rj6IF9w7SLnW6vk/sAWWedeidY4jvk7GfsT729GtecncFzzyYqVVZQ2itqDK3S3ecFFyQdNVuCDapG7liSSSTvJqPFOKS0kPw9bzPH5pe99vf5LGPdq8S4lLSx9hB5n1fr0jx2uSTvPs+4UIxBuCQRuI0I9tKdm7PkncRxIWY9m4DtJ3AemrM5M8jY8NaSS0kvb5qH5oO8/OP3V5jT6Wy95XLuef0uit1Msrl3frcr/ZPgtkxeMOLxJMUBMbBBYSSsEXMT8QFgTc6nXQXDVcWDwiRII40CqNwH+9T31vor0sK1FLv3PZb4jndpYyFFFFTMhXK3KWIvtHFqN7YvEAekzvXVNcu7bmEe08VIdy4vEn/ADXrDNijrgXcpZ1SCPDrowYZx3Bf62+o02cnuT2Ix0vNYePMR1mOiIDxd+Ho1J4A1M+Svg7n2g4xOLvBC2trWlkHcD1FPxjqeA1vV0bJ2VDhYlhgjWONdyjt7STqSeJNyaxjJNWquOFzI1yJ8H2HwFpD8NiOMrDRO6NfNHfvOuttKmNFFSNaUnJ5YUUUUIhRRRQBRRRQBRRRQBRRRQBRRRQBRRRQBRRRQBRRRQBRRRQCOLyiT1UXvS1jD5TL6qH35qyi8ok9VF70tYw+Uy+qh9+agF1YSyBRmYgAcT9Vap8TY5VGZ/ijh2Fm80febGwNq8iw2udzmYbuCrw6K/XqddTw0oBg5cO74KZrZU6FgR0m+ESxI80cbb917WINMco5smGbtkYIvoHSY/co/wCurv5e+QTfs/4qVTmM2E2LbDqlmCs+aMHpsSV0UDcCqjpGwFj6K0V7NcQrla9ks/qs4+pybVX/ADGErHso5/XLwadkQ5MPCvaudvpOcwP/AGZB7Kl3JrkhLirO944fjEdJh8wf+R09NSjk7yIVCJcTZ5N4jHUX0/GI7Nw799TOqVo3fdK+7q28fcqhw533Svv6vOPLIj2XsyLDpzcSBRx7WPax4mllFFdNRUVhHajFRWEsIKKKKyZCiiigCoVsPwdYeLFTYya00sk8kqBh0Is0jOLLxcXHSO7gBvM1ooSUmuQUUUUIhRRRQBRRRQBRRRQBRRRQBRRRQBRRRQBRRRQBRRRQBRRRQBRRRQBRRRQDdiZhFKXZowJFRFzOFJZWc2AI1643VqgzSTSMroFskblGDuroXJW1rKfhF33PdqDW/a2CeYKiuFTNeVSpbnFt1LhhYE2vvuLjcTWWAwbI87M6sJZA6gKVK2jSOxOY5uoDew3mgFMEKoLKLD6yT2knUnvNbKKKAbeUWzTicO8AYLnKdIi9gHVjpx0BrzYewocKuWNdT1nOrN6T2dw0pzoqv2cfH48b8iv2MHZ7RrflkKKKKsLAooooAooooAooooD/2Q==)\n","\n","## Deconvolution, Transposed Conv and Upsampling\n","\n","\n","\n","\n","https://distill.pub/2016/deconv-checkerboard/\n","\n","When we have neural networks generate images, we often have them build them up from low resolution, high-level descriptions. This allows the network to describe the rough image and then fill in the details.\n","\n","In order to do this, we need some way to go from a lower resolution image to a higher one. We generally do this with the deconvolution operation. Roughly, deconvolution layers allow the model to use every point in the small image to “paint” a square in the larger one.\n","\n","(Deconvolution has a number of interpretations and different names, including “transposed convolution.”\n","\n","\n","http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html\n","https://github.com/vdumoulin/conv_arithmetic\n","\n","__How to get larger feature maps out of small ones?__\n","Let’s now consider what would be required to go the other way around, i.e., map from a 4-dimensional space to a 16-dimensional space. This operation is known as a transposed convolution.\n","Sometimes, it is referred to as \"fractionally strided convolutions\", becasue if you imagine you have stride S=1/2, then you enlarge the size by 2.\n","\n","The simplest way is to apply a kernel that is larger in size than the input, and use __normal__ conv:\n","\n","Convolving kernel N=3 with input M=2 gives N+M-1=4 --> 4x4 output\n","\n","![Deconv](http://deeplearning.net/software/theano/_images/no_padding_no_strides.gif)\n","\n","But what if we have a large input, like 100x100, and we want to have bigger output.\n","In this case we can pad the input:\n","\n","\n","![Deconv_pad](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides_transposed.gif)\n","\n","__Fraction strides__\n","transpose of a convolution with s > 1 involves an equivalent convolution with s < 1. As will be explained, this is a valid intuition, which is why transposed convolutions are sometimes called fractionally strided convolutions.\n","\n","![Strided_deconv](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_strides_transposed.gif)\n","\n","\n","## Dialated Conv\n","\n","Those familiar with the deep learning literature may have noticed the term “dilated convolutions” (or “atrous convolutions”, from the French expression convolutions à trous) appear in recent papers. Here we attempt to provide an intuitive understanding of dilated convolutions. For a more in-depth description and to understand in what contexts they are applied, see Chen et al. (2014) [2]; Yu and Koltun (2015) [3].\n","\n","Dilated convolutions “inflate” the kernel by inserting spaces between the kernel elements. The dilation “rate” is controlled by an additional hyperparameter d. Implementations may vary, but there are usually d - 1 spaces inserted between kernel elements such that d = 1 corresponds to a regular convolution.\n","\n","![Dilated](http://deeplearning.net/software/theano/_images/dilation.gif)\n","\n","Dialated conv is good with sparse inputs, where we don't need to multiply the kernel with evey input, but we want a bigger receptive field, keeping the same number of kernel params.\n","\n","## Upsampling:\n","https://distill.pub/2016/deconv-checkerboard/\n","\n","__What if (N-M+1)/S is a fraction?__\n","Unfortunately, deconvolution can easily have “uneven overlap,” putting more of the metaphorical paint in some places than others [7]. In particular, deconvolution has uneven overlap when the kernel size (the output window size) is not divisible by the stride (the spacing between points on the top). While the network could, in principle, carefully learn weights to avoid this  — as we’ll discuss in more detail later — in practice neural networks struggle to avoid it completely.\n","\n","To avoid these artifacts, we’d like an alternative to regular deconvolution (“transposed convolution”). Unlike deconvolution, this approach to upsampling shouldn’t have artifacts as its default behavior. Ideally, it would go further, and be biased against such artifacts. Upsample is simply the opposite of max pool. It can spread based on NN pixels, or using bilinear transforms.\n","\n","__Upsample + Conv__\n","Another approach is to separate out upsampling to a higher resolution from convolution to compute features. For example, you might resize the image (using nearest-neighbor interpolation or bilinear interpolation) and then do a convolutional layer. This seems like a natural approach, and roughly similar methods have worked well in image super-resolution\n","\n","\n","\n","## Encoder Decoder Architecture\n","![EncDec](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/583e2dab221ad5d54c1b1cc0a9df4f1254bf3942/3-Figure1-1.png)\n","\n","\n","___Downsampling___\n","\n","__Stride 2 conv:\n","Using stride 2 means the width and height of the feature map are downsampled by a factor of 2 (in addition to any changes induced by border effects).\n","So in SAME conv (the most widely used type, we halve the size by stride 2 conv__\n","\n","According to the equations (valid: (N-M+1)/S, normal: (N-M+2P)/S + 1) stride 2 conv gives roughtly half the size:\n","\n","![strid_2](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_strides.gif)\n","\n","\n","_Strided convolutions are rarely used in practice_, although they can come in handy for some types of models; it’s good to be familiar with the concept.\n","\n","To downsample feature maps, instead of strides, we tend to use the __max-pooling__ operation, which you saw in action in the first convnet example. Let’s look at it in more depth.\n","\n","According to the equations (valid: (N-M+1)/S, normal: (N-M+2P)/S + 1) stride 2 conv gives roughtly half the size:\n","\n","![strid_2](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_strides.gif)\n","\n","# The max-pooling operation\n","\n","_Downsampleing_: In the convnet example, you may have noticed that the size of the feature maps is halved after every MaxPooling2D layer.\n","\n","For instance, before the first MaxPooling2D layers, the feature map is 26 × 26, but the max-pooling operation halves it to 13 × 13.\n","\n","_That’s the role of max pooling: to aggressively downsample feature maps, much like strided convolutions._\n","\n","Max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel.\n","\n","_It’s conceptually similar to convolution, except that instead of transforming local patches via a learned linear transformation (the convolution kernel), they’re transformed via a hardcoded max tensor operation._\n","\n","_A big difference from convolution is that max pooling is usually done with 2 × 2 windows and stride 2, in order to downsample the feature maps by a factor of 2. On the other hand, convolution is typically done with 3 × 3 windows and no stride (stride 1)._\n","\n","__Average Pooling__\n","\n","https://medium.com/@bdhuma/which-pooling-method-is-better-maxpooling-vs-minpooling-vs-average-pooling-95fb03f45a9\n","\n","Note that: the blog has issue in plotting, we need to add .astype(uint8)\n","\n","\n","\n","Max pooling selects the brighter pixels from the image. It is useful when the background of the image is dark and we are interested in only the lighter pixels of the image. For example: in MNIST dataset, the digits are represented in white color and the background is black. So, max pooling is used. Similarly, min pooling is used in the other way round.\n","\n","Whereas average pooling extracts features like edges so smoothly.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VRh5GFV77oqx"},"source":["# CAMVID\n","[CAMVID](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/) is dataset by Cambridge\n","_\"The Cambridge-driving Labeled Video Database (CamVid) is the first collection of videos with object class semantic labels, complete with metadata. The database provides ground truth labels that associate each pixel with one of 32 semantic classes.\"_\n","\n","It has 701 RGB images (960x720), and their corresponding ground truth masks.\n","It has 32 classes, where each pixel is labeled according to 1 of 32 colors. This is called per-pixel semantic segmentation (vs. instance segmentation as described above).\n","\n","The colors mapping can be found in a separate [file](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/data/label_colorsSorted.txt).\n","\n","The ground truth are represented as colored images, where each pixel color corresponds to a class according to the mapping in the file.\n","\n","To generate the masks for training, you need to perform this mapping yourself. Once done, you can save them as npy files.\n"]},{"cell_type":"markdown","metadata":{"id":"xkKSReOQAScl"},"source":["To get all the files, you can get the labels (masks images) from this [link](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/data/LabeledApproved_full.zip)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"AfBUsf6M38si","executionInfo":{"status":"error","timestamp":1749114209600,"user_tz":-180,"elapsed":121096,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}},"outputId":"23635bc2-1a8f-485c-b2cc-c853b356351f"},"execution_count":1,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"mount failed","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         )\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed"]}]},{"cell_type":"markdown","metadata":{"id":"_0jfcpCpActu"},"source":["However, to get the corresponding images, you need to run a script to extract the frames from few video sequences (like CamSeq01)\n","\n","Under this nice [repo](https://github.com/GeorgeSeif/Semantic-Segmentation-Suite), the author has already done that for us, and split the data into train/val/test already. We're going to use that."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iVAkzWmrH43N","executionInfo":{"status":"aborted","timestamp":1749114209794,"user_tz":-180,"elapsed":228,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["view = 0\n","batch_sz = 4\n","epochs = 1\n","steps_per_epoch = 1000\n","validation_steps = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gov_3_yWc_29","executionInfo":{"status":"aborted","timestamp":1749114209803,"user_tz":-180,"elapsed":236,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["!git clone https://github.com/GeorgeSeif/Semantic-Segmentation-Suite.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiPEYNh19Nqw","executionInfo":{"status":"aborted","timestamp":1749114209808,"user_tz":-180,"elapsed":239,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIm4Uo4IJRux","executionInfo":{"status":"aborted","timestamp":1749114209813,"user_tz":-180,"elapsed":243,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["from pathlib import Path\n","data_path = Path('Semantic-Segmentation-Suite/CamVid')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hj6dRP8c9zFI","executionInfo":{"status":"aborted","timestamp":1749114209819,"user_tz":-180,"elapsed":121988,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["print('Number of train frames: ' + str(len(os.listdir(data_path/'train'))))\n","print('Number of train labels: ' + str(len(os.listdir(data_path/'train_labels'))))\n","print('Number of val frames: ' + str(len(os.listdir(data_path/'val'))))\n","print('Number of val labels: ' + str(len(os.listdir(data_path/'val_labels'))))\n","print('Number of test frames: ' + str(len(os.listdir(data_path/'test'))))\n","print('Number of test labels: ' + str(len(os.listdir(data_path/'test_labels'))))\n","print('Total frames: ' + str(len(os.listdir(data_path/'train')) + len(os.listdir(data_path/'val')) + len(os.listdir(data_path/'test'))))"]},{"cell_type":"markdown","metadata":{"id":"7C6OliBmA0rP"},"source":["Now, let's see which classes we have. This can be found in the original CAMVID [text file](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/data/label_colors.txt). However, under the same repo, the author has dumped it into csv which we will use."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FKdmFX1DKDud","executionInfo":{"status":"aborted","timestamp":1749114209823,"user_tz":-180,"elapsed":121985,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["import pandas as pd\n","classes = pd.read_csv(data_path / 'class_dict.csv', index_col =0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jiyA9Aa0KMZl","executionInfo":{"status":"aborted","timestamp":1749114209827,"user_tz":-180,"elapsed":121979,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yH17IqAxGdN4","executionInfo":{"status":"aborted","timestamp":1749114209832,"user_tz":-180,"elapsed":121973,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["n_classes = len(classes)\n","n_classes"]},{"cell_type":"markdown","metadata":{"id":"WYOiawzIBHWC"},"source":["This data frame maps the class names to colors.\n","\n","To access the colors, we can index the dataframe with its row index name using the .loc operation.\n"]},{"cell_type":"markdown","metadata":{"id":"rMWbSrv2B3Ns"},"source":["Now we are ready to create a map from class name to color"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u9QZkCt87fAj","executionInfo":{"status":"aborted","timestamp":1749114209837,"user_tz":-180,"elapsed":121970,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["cls2rgb = {cl:list(classes.loc[cl, :]) for cl in classes.index}\n","cls2rgb"]},{"cell_type":"markdown","metadata":{"id":"8BAQSaWvGirG"},"source":["## Now let's visualize and explore some samples:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q7f1_yHcm3Uh","executionInfo":{"status":"aborted","timestamp":1749114209843,"user_tz":-180,"elapsed":121965,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["%matplotlib inline\n","import cv2\n","import matplotlib.pyplot as plt\n","#from google.colab.patches import  cv2_imshow\n","\n","#img = cv2.imread(data_path/'train/0001TP_006690.png')\n","img = cv2.imread(str(data_path) + '/train/0001TP_006690.png')\n","plt.imshow(img)\n"]},{"cell_type":"markdown","metadata":{"id":"kpJ4GpiuGvvB"},"source":["Let's have a look on the masks (the ground truth)"]},{"cell_type":"markdown","metadata":{"id":"yQPgxveDG1P7"},"source":["As you can see the masks are just colors (L,W,3).\n","What we actually want is a (L,W) matrix, with each value is from 0 to 31 representing the 32 class labels."]},{"cell_type":"markdown","metadata":{"id":"nMBOk3-3QuLr"},"source":["Colors are different from the colors in cls2rgb! Because the order is BGR not RGB when using cv2.imread: https://stackoverflow.com/questions/46898979/how-to-check-the-channel-order-of-an-image\n","\n","If you want to get the same order as in the color mapping of CAMVID, use the cv converted"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7dalUjddkDEb","executionInfo":{"status":"aborted","timestamp":1749114209846,"user_tz":-180,"elapsed":121954,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["import numpy as np\n","mask = cv2.imread(str(data_path) + '/train_labels/0001TP_006690_L.png')\n","mask = cv2.cvtColor((mask).astype(np.uint8), cv2.COLOR_BGR2RGB)# If you want to get the same order as in the color mapping of CAMVID, use the cv converted"]},{"cell_type":"markdown","metadata":{"id":"C533H1FPQ_Uh"},"source":["Now if you plot the mask again, you will see different colors. For example the red and blue are reversed than before:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mU_LOU2tkF0F","executionInfo":{"status":"aborted","timestamp":1749114209853,"user_tz":-180,"elapsed":121946,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["plt.imshow(mask)"]},{"cell_type":"markdown","metadata":{"id":"FcY60Jg4RSl7"},"source":["Another solution is to use load_image from keras which uses RGB (it uses PIL under the hood) unlike cv2.imread"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FPnvFddHi0z7","executionInfo":{"status":"aborted","timestamp":1749114209856,"user_tz":-180,"elapsed":121908,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["from keras.preprocessing.image import load_img\n","mask = load_img(str(data_path) + '/train_labels/0001TP_006690_L.png')\n","mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7QYCJFNBjB0-","executionInfo":{"status":"aborted","timestamp":1749114209859,"user_tz":-180,"elapsed":121895,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["mask = np.array(mask)# Now colors are the same as in the dict, since keras load_img uses RGB order."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZEkNOms5rOFc","executionInfo":{"status":"aborted","timestamp":1749114209863,"user_tz":-180,"elapsed":121884,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["mask.shape"]},{"cell_type":"markdown","metadata":{"id":"COTqVs7nRfGk"},"source":["# Masks labels adjustment\n","Now to perform the mapping, we want to apply a mapping using on every pixel.\n","We can create a dict from color to indx (rgb2idx) and apply it on all pixels.\n","It can be done using the apply_along_axis operation, but takes very long.\n","\n","The NN model classification output is usually produced from a softmax. The softmax produces a probability score over n_classes output. If we perform max over this output, we have a vector of size n_classesx1.\n","\n","As described earlier we have LxW outputs, one per pixel, each representing a class, which we called a class heatmap. In NN world, this output is produced from LxW softmax operations over each pixel, so we have LxWxn_classes scores.\n","\n","The ground truth for this output must also have LxWxn_classes, where the encoding of class label is done via One-Hot-Encoding (OHE).\n","\n","In theory, the LxW mask is enough to train a NN model, since we can use sparse_categorical_cross_entropy. But we will not use this option for reasons that will be described later.\n","\n","\n","\n","So we will try now to produce the LxWxn_classes mask:\n","__Numpy vector operations__\n","Below is a much faster implementation.\n","\n","It loops over every class color, and looks for it in the mask. If found, it sets its value at the corresponding pixel, else, the pixel mask value stays all zeros. The logical operations over the whole image is much faster, while looping is kept only on the few classes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2h36dbNQ8Jvj","executionInfo":{"status":"aborted","timestamp":1749114209872,"user_tz":-180,"elapsed":121873,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["def adjust_mask(mask, flat=False):\n","\n","    semantic_map = []\n","    for colour in list(cls2rgb.values()):\n","        equality = np.equal(mask, colour)# 256x256x3 with True or False\n","        class_map = np.all(equality, axis = -1)# 256x256 If all True, then True, else False\n","        semantic_map.append(class_map)# List of 256x256 arrays, map of True for a given found color at the pixel, and False otherwise.\n","    semantic_map = np.stack(semantic_map, axis=-1)# 256x256x32 True only at the found color, and all False otherwise.\n","    if flat:\n","      semantic_map = np.reshape(semantic_map, (-1,256*256))\n","\n","    return np.float32(semantic_map)# convert to numbers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-msSAjN8Li6","executionInfo":{"status":"aborted","timestamp":1749114209876,"user_tz":-180,"elapsed":121872,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["new_mask = adjust_mask(mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rvAW7ymK_GdT","executionInfo":{"status":"aborted","timestamp":1749114209879,"user_tz":-180,"elapsed":121861,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["new_mask.shape"]},{"cell_type":"markdown","metadata":{"id":"0cnX1U6KSL3P"},"source":["To make sure we performed the mapping correctly, let's try to do the reverse mapping.\n","Note that, we will need this later when we get predictions from our network and we want to view it as RGB mask like the original data labels.\n","\n","First, we want to create the reverse mapping from class index to RGB color. Again we use dict comprehensions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DHXtvDcEw7Wc","executionInfo":{"status":"aborted","timestamp":1749114209885,"user_tz":-180,"elapsed":121850,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["idx2rgb={idx:np.array(rgb) for idx, (cl, rgb) in enumerate(cls2rgb.items())}\n","idx2rgb"]},{"cell_type":"markdown","metadata":{"id":"mSxvEhfxSjP1"},"source":["This time we have a (L,W,32) mask and we want to map it to color mask (L,W,3). First we need to map it to index map using np.argmax, then use idx2rgb to restore the color back"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JcWc882tw7Wg","executionInfo":{"status":"aborted","timestamp":1749114209889,"user_tz":-180,"elapsed":121838,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["# Map the idx back to rgb\n","def map_class_to_rgb(p):\n","  return idx2rgb[p[0]]\n","\n","\n","print(new_mask.shape)\n","rgb_mask = np.apply_along_axis( map_class_to_rgb, -1, np.expand_dims(np.argmax(new_mask, axis=-1), -1))\n","\n","print(np.expand_dims(np.argmax(new_mask, axis=-1), -1).shape)\n","print(rgb_mask.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-XwnT2wjw7Wj","executionInfo":{"status":"aborted","timestamp":1749114209897,"user_tz":-180,"elapsed":121825,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["plt.imshow(rgb_mask)"]},{"cell_type":"markdown","metadata":{"id":"iJSV7sKb8kUm"},"source":["# Model\n","__U-Net__\n","\n","We will investigate a famous model called U-Net\n","\n","![U Net](https://lmb.informatik.uni-freiburg.de/Publications/2015/RFB15a/u-net-architecture.png)\n","\n","It uses the same architecture as the encoder decoder. However it adds a well known trick called skip connections. Skip connections was first introduced in ResNet. It roots also in the recurrent neural network literature with LSTMs and GRUs.\n","\n","The main issue skip connections is trying to solve is the very deep nets. Or in recurrence, the very long time dependency (back prop through time). With the increased network depth, we face the problem of vanishing gradients, due to the chain rule effect, the gradients are reduced a lot that they could vanish, and no weight updates is possible.\n","\n","Skip connections enables an alternative path to the gradients:\n","\n","`x2=f(x1) + x1`\n","\n","This is called a residual block\n","\n","![Res](https://qph.fs.quoracdn.net/main-qimg-93cef3d493d15b211aba8db3fd536b82)\n","\n","In this way, if the gradient of F(x) vanished, we have an identity gradient of x that can still flow.\n","\n","\n","Also, U-Net uses upsampling as we discussed earlier.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pg5d04fmsQXu","executionInfo":{"status":"aborted","timestamp":1749114209901,"user_tz":-180,"elapsed":121818,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["import numpy as np\n","import os\n","#import skimage.io as io\n","#import skimage.transform as trans\n","import numpy as np\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras import backend as keras\n","\n","\n","def unet(n_classes, pretrained_weights = None,input_size = (256,256,3), flat=False, ohe=True):\n","    inputs = Input(input_size)\n","    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n","    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n","    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n","    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n","    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n","    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n","    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n","    drop4 = Dropout(0.5)(conv4)\n","    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n","\n","    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n","    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n","    drop5 = Dropout(0.5)(conv5)\n","\n","    x = UpSampling2D(size = (2,2))(drop5)\n","    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(x)\n","    merge6 = concatenate([drop4,up6], axis = 3)\n","    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n","    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n","\n","    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n","    merge7 = concatenate([conv3,up7], axis = 3)\n","    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n","    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n","\n","    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n","    merge8 = concatenate([conv2,up8], axis = 3)\n","    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n","    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n","\n","    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n","    merge9 = concatenate([conv1,up9], axis = 3)\n","    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n","    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n","    #conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n","    #conv10 = Conv2D(n_classes, (1,1), activation = 'softmax')(conv9)\n","    conv10 = Conv2D(n_classes, (1,1), padding='same')(conv9)\n","    if flat:\n","      output_layer = Reshape((256*256,n_classes))(conv10)\n","    else:\n","      output_layer = conv10\n","    output_layer = Activation('softmax')(output_layer)\n","\n","\n","    model = Model(inputs = inputs, outputs = output_layer)\n","\n","    if ohe:\n","      model.compile(optimizer = Adam(learning_rate = 1e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","    else:\n","      model.compile(optimizer = Adam(learning_rate = 1e-4), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n","\n","    #model.summary()\n","\n","    if(pretrained_weights):\n","        model.load_weights(pretrained_weights)\n","\n","    return model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jg46Yd6K73bl","executionInfo":{"status":"aborted","timestamp":1749114209904,"user_tz":-180,"elapsed":121807,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["model = unet(n_classes,input_size=(128,128,3))\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"4W6S1F1ASGwS"},"source":["# Data in RAM\n","One option is to perform the above operation over all the data files masks, and load them all in RAM.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1vQ28MaWwZwT"},"source":["You must sort the files, otherwise you get wrong GT to mask mapping"]},{"cell_type":"markdown","metadata":{"id":"qwAic38OxJe-"},"source":["You don't need to get the same file name, in the 2 folders, as we can sort by name. The only difference in img and mask name is _L.png"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"miHfhtkMSGPP","executionInfo":{"status":"aborted","timestamp":1749114209908,"user_tz":-180,"elapsed":121802,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["def load_CAMVID(data_type='train',target_size_=(128,128), enc='ohe', shape='normal'):\n","  img_path = str(data_path) + '/' + data_type + '/'\n","  labels_path = str(data_path) + '/' + data_type + '_labels/'\n","  # without adding target_size=(256,256) in load_img we get Out of mem: 421x960x720x32x4bytes is around 34GB!\n","  x = np.array([np.array(load_img(str(img_path) + file, target_size=target_size_))*1./255 for file in sorted(os.listdir(img_path))])\n","  if(enc=='ohe'):\n","\n","    y = np.array([adjust_mask(np.array(load_img(str(labels_path) + file, target_size=target_size_))) for file in sorted(os.listdir(labels_path))])\n","  elif(enc=='sparse_cat'):\n","    y = np.array([adjust_mask(np.array(load_img(str(labels_path) + file, target_size=target_size_))) for file in sorted(os.listdir(labels_path))])\n","  if(shape == 'flat'):\n","    y = np.reshape(y.shape[0], y.shape[1]*y.shape[2])\n","    y = np.expand_dims(y, axis=-1)\n","  return x, y\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqyL9si8TkVj","executionInfo":{"status":"aborted","timestamp":1749114209953,"user_tz":-180,"elapsed":121839,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["import time\n","start = time.time()\n","x_train, y_train = load_CAMVID(data_type='train')  # can not load all data in ram\n","#x_test, y_test = load_CAMVID(data_type='test')# Don't load test for RAM consumption\n","x_val, y_val = load_CAMVID(data_type='val')\n","end = time.time()\n","print('Time elapsed: ', end-start)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXYhotFdT9J9","executionInfo":{"status":"aborted","timestamp":1749114209962,"user_tz":-180,"elapsed":121838,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["print(x_train.shape)\n","print(y_train.shape)\n","#print(x_test.shape)\n","#print(y_test.shape)\n","print(x_val.shape)\n","print(y_val.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"WsQL-8xw8yYC"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMoTMzUxUo9S","executionInfo":{"status":"aborted","timestamp":1749114209969,"user_tz":-180,"elapsed":121837,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["model_checkpoint = ModelCheckpoint('unet_camvid_fromd_Ram_Data.keras', monitor='val_loss',verbose=1, save_best_only=True)\n","model.fit(x=x_train,\n","              y=y_train,\n","              validation_data=(x_val, y_val),\n","              batch_size=10,# 32 gives OOM sometimes i change it to 5 instade of 32\n","              epochs=epochs,\n","              callbacks=[model_checkpoint])"]},{"cell_type":"code","source":["import shutil\n","\n","# Copy from Colab local storage to Google Drive\n","shutil.copy('unet_camvid_fromd_Ram_Data.keras',\n","            '/content/drive/MyDrive/ColabNotebooks/DI_Dr_sallab_youtube/ved_7/unet_camvid_fromd_Ram_Data.keras')\n"],"metadata":{"id":"RtKD7-TD7-5j","executionInfo":{"status":"aborted","timestamp":1749114209973,"user_tz":-180,"elapsed":121828,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HY5WythVpDTf"},"source":["# Let's try on some samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PsYvUuSpS0U","executionInfo":{"status":"aborted","timestamp":1749114209976,"user_tz":-180,"elapsed":121823,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["# img (256,256,3)\n","# gt_mask: gt_mode=sparse--> (256,256) or ohe --> (256,256,32)\n","def visualize_seg(img, gt_mask, shape='normal', gt_mode='sparse'):\n","  plt.figure(1)\n","\n","  # Img\n","  plt.subplot(311)\n","  plt.imshow(img)\n","\n","  # Predict\n","  pred_mask = model.predict(np.expand_dims(img, 0))\n","  pred_mask = np.argmax(pred_mask, axis=-1)  # shape 960*701*1\n","  pred_mask = pred_mask[0]\n","  print(pred_mask)\n","  if shape=='flat':\n","    pred_mask = np.reshape(pred_mask, (256,256)) # Reshape only if you use the flat model. O.w. you dont need\n","\n","  rgb_mask = np.apply_along_axis(map_class_to_rgb, -1, np.expand_dims(pred_mask, -1))\n","\n","  # Prediction\n","  plt.subplot(312)\n","  plt.imshow(rgb_mask)\n","\n","  # GT mask\n","  if gt_mode == 'ohe':\n","    gt_img_ohe = np.argmax(gt_mask, axis=-1)\n","    gt_mask = np.apply_along_axis(map_class_to_rgb, -1, np.expand_dims(gt_img_ohe, -1))\n","\n","  plt.subplot(313)\n","  plt.imshow((gt_mask).astype(np.uint8))\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDAr8irfq_r5","executionInfo":{"status":"aborted","timestamp":1749114209979,"user_tz":-180,"elapsed":121822,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["visualize_seg(x_val[100], y_val[100], gt_mode='ohe')"]},{"cell_type":"markdown","metadata":{"id":"cqaegmGmB7MR"},"source":["# Load data from disk\n","\n","We will explore two approaches, the first is to build our own custom loader. This method is generic, and works for both image and other data sets.\n","\n","Then we will see how Keras provides a convenient way to do that for Image data.\n","\n","## Custom Data loaders\n","Keras provides a basic generator to inherity from `keras.utils.Sequence`, we just need to implement `len` and `getitem` methods."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSRXS0mZUGQm","executionInfo":{"status":"aborted","timestamp":1749114209987,"user_tz":-180,"elapsed":121825,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["from keras.utils import Sequence\n","class CAMVID_Dataset(Sequence):\n","\n","\n","    def __init__(self, data_path, batch_size=4, dim=(256,256), n_classes=32, data_type='train', shape='normal'):\n","\n","        self.images_dir = str(data_path) + '/' + data_type + '/'\n","        self.masks_dir = str(data_path) + '/' + data_type + '_labels/'\n","        assert len(os.listdir(self.images_dir)) == len(os.listdir(self.masks_dir))\n","        self.data_type = data_type\n","        self.shape = shape\n","        self.batch_size = batch_size\n","        self.dim = dim\n","        self.n = len(os.listdir(self.images_dir))\n","        self.n_batches = int(np.floor(self.n  / self.batch_size))\n","        self.indexes = np.arange(self.n)\n","\n","    def __len__(self):\n","        return  self.n_batches\n","\n","    def __getitem__(self, index):\n","        X = np.empty((self.batch_size, *self.dim, 3))\n","        Y = np.zeros((self.batch_size, *self.dim, n_classes))\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Generate data\n","        for i, ID in enumerate(indexes):\n","          idx = ID\n","\n","          file = sorted(os.listdir(self.images_dir))[idx]\n","\n","          # Load image\n","          image = np.array(load_img(str(self.images_dir) + file, target_size=(256,256)))*1./255\n","\n","\n","          # Load mask\n","          file = sorted(os.listdir(self.masks_dir))[idx]\n","          mask = adjust_mask(np.array(load_img(str(self.masks_dir) + file, target_size=(256,256))))\n","\n","          if(self.shape == 'flat'):\n","            mask = np.reshape(mask.shape[0], mask.shape[1]*mask.shape[2])\n","            mask = np.expand_dims(mask, axis=-1)\n","          X[i,:] = image\n","          Y[i,:] = mask\n","\n","        return X, Y\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bHmJVO4yVsLy","executionInfo":{"status":"aborted","timestamp":1749114209992,"user_tz":-180,"elapsed":121825,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["train_gen = CAMVID_Dataset(str(data_path), batch_size=batch_sz, n_classes=n_classes, data_type='train')\n","valid_gen = CAMVID_Dataset(str(data_path), batch_size=batch_sz, n_classes=n_classes, data_type='val')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"av0UbltOZYYj","executionInfo":{"status":"aborted","timestamp":1749114209996,"user_tz":-180,"elapsed":121825,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["x,y = next(enumerate(train_gen))[1]\n","print(x.shape)\n","print(y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCRWbGVoVsnc","executionInfo":{"status":"aborted","timestamp":1749114210002,"user_tz":-180,"elapsed":121827,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["n_train_samples = len(os.listdir(str(data_path) + '/train/'))\n","n_train_samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLDvJSIVXm60","executionInfo":{"status":"aborted","timestamp":1749114210005,"user_tz":-180,"elapsed":121825,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["\n","model = unet(n_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2yD5z0ZXo8_","executionInfo":{"status":"aborted","timestamp":1749114210009,"user_tz":-180,"elapsed":121825,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["model_checkpoint = ModelCheckpoint('unet_camvid_from_disk_data.keras', monitor='val_loss',verbose=1, save_best_only=True)\n","model.fit(train_gen,\n","                    validation_data=valid_gen,\n","                    steps_per_epoch=n_train_samples,\n","                    validation_steps=validation_steps,\n","                    epochs=epochs,\n","                    callbacks=[model_checkpoint])"]},{"cell_type":"code","source":["import shutil\n","\n","# Copy from Colab local storage to Google Drive\n","shutil.copy('unet_camvid_from_disk_data.keras',\n","            '/content/drive/MyDrive/ColabNotebooks/DI_Dr_sallab_youtube/ved_7/unet_camvid_from_disk_data.keras')\n"],"metadata":{"id":"8aMF5jTB8-wh","executionInfo":{"status":"aborted","timestamp":1749114210013,"user_tz":-180,"elapsed":121824,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","print(os.getcwd())\n"],"metadata":{"id":"jSpGhW2f3rvl","executionInfo":{"status":"aborted","timestamp":1749114210019,"user_tz":-180,"elapsed":121825,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","print(os.listdir('.'))\n"],"metadata":{"id":"dVQp5bKz4dnb","executionInfo":{"status":"aborted","timestamp":1749114210024,"user_tz":-180,"elapsed":121822,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"74icmkUdXxSw"},"source":["Next we will see how this can be automated using ImageDataGenerator, and also to include data augmentation.\n","\n","_Note_ all the next steps can also be done using custom class as above."]},{"cell_type":"markdown","metadata":{"id":"v2IpG69obYa7"},"source":["## Using ImageDataGenerator\n","\n","We won't specify any data augmentation for now.\n","\n","But if we were to, we need to take care of few things:\n","\n","1- __Fix the seed__ we dont want to perform an operation on the image different from the mask. But in the same time, we have two generators, one for mask and one for images (as they are different files on disk)\n","\n","2- __How to ensure the label and image files correspond to each others?__ We will have mask and image generators (loaders), each points to a different folder. Moreover, the file names of the img and mask are not the same. When we loaded the data in RAM ourselves, we used sorted to ensure consistency.\n","\n","In keras, sorted is used as well. See [here](https://stackoverflow.com/questions/42868982/how-do-i-check-the-order-in-which-keras-flow-from-directory-method-processes-fo)\n","\n","\n","3- When speficifying the directory to flow_from_directory, the function assumes every sub-directory corresponds to a class (see cats vs dogs tutorial). If this is not the case, no files are loaded. However, we are not doing image classification this is not applicable.\n","To workaround this, we use the train, val, test folders as if they are class folders, using the parameter `classes`\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121819,"status":"aborted","timestamp":1749114210028,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"SBaHaQaVBkPp"},"outputs":[],"source":["# Data generator\n","#https://keras.io/preprocessing/image/\n","# Data generator\n","#batch_sz = 4\n","\n","from keras.preprocessing.image import ImageDataGenerator\n","# we create two instances with the same arguments\n","\n","# VI Note: use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n","data_gen_args = dict(rescale=1./255)\n","\n","# So our usage here is as data loader instead of loading everything in RAM, not data augmentation\n","mask_gen_args = dict()\n","\n","image_datagen = ImageDataGenerator(**data_gen_args)\n","mask_datagen  = ImageDataGenerator(**mask_gen_args)\n","\n","# Provide the same seed and keyword arguments to the fit and flow methods\n","seed = 1\n","#image_datagen.fit(images, augment=True, seed=seed)\n","#mask_datagen.fit(masks, augment=True, seed=seed)\n","\n","image_generator = image_datagen.flow_from_directory(\n","    data_path,\n","    class_mode=None,\n","    classes=['train'],\n","    seed=seed,\n","    batch_size=batch_sz,\n","    target_size=(256,256))\n","\n","mask_generator = mask_datagen.flow_from_directory(\n","    data_path,\n","    classes=['train_labels'],\n","    class_mode=None,\n","    seed=seed,\n","    color_mode='rgb',\n","    batch_size=batch_sz,\n","    target_size=(256,256))\n","\n","# combine generators into one which yields image and masks\n","train_generator = zip(image_generator, mask_generator)\n","\n","\n","\n","val_image_generator = image_datagen.flow_from_directory(\n","    data_path,\n","    class_mode=None,\n","    classes=['val'],\n","    seed=seed,\n","    batch_size=batch_sz,\n","    target_size=(256,256))\n","\n","val_mask_generator = mask_datagen.flow_from_directory(\n","    data_path,\n","    classes=['val_labels'],\n","    class_mode=None,\n","    seed=seed,\n","    batch_size=batch_sz,\n","    color_mode='rgb',\n","    target_size=(256,256))\n","\n","# combine generators into one which yields image and masks\n","val_generator = zip(val_image_generator, val_mask_generator)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggvo-00qCHdu","executionInfo":{"status":"aborted","timestamp":1749114210036,"user_tz":-180,"elapsed":121826,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["\n","def train_generator_fn():\n","\n","    for (img,mask) in train_generator:\n","        new_mask = adjust_mask(mask)\n","        yield (img,new_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V5uqwTCvCI2v","executionInfo":{"status":"aborted","timestamp":1749114210042,"user_tz":-180,"elapsed":121831,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["\n","def val_generator_fn():\n","\n","    for (img,mask) in val_generator:\n","        new_mask = adjust_mask(mask)\n","        yield (img,new_mask)"]},{"cell_type":"markdown","metadata":{"id":"ihriMWvWf8AU"},"source":["_steps_per_epoch: _\n","\n","Remember we dont have constant data size due to augmentation and yield in the generator. If you dont have augmentation and just using flow_from_directory as a data loader, then set this to size/batch_size_\n","\n","Note that, since we don't use data augmentation, we have fixed data size. In this case steps_per_epoch is just the total_size/batch_size"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121829,"status":"aborted","timestamp":1749114210047,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"_daKGC16FRtr"},"outputs":[],"source":["n_train_samples = len(os.listdir(str(data_path) + '/train/'))\n","n_train_samples"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121831,"status":"aborted","timestamp":1749114210055,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"wi67xiPWMiaa"},"outputs":[],"source":["\n","model = unet(n_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121832,"status":"aborted","timestamp":1749114210062,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"IVuzPjyU8xJm"},"outputs":[],"source":["model_checkpoint = ModelCheckpoint('unet_camvid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n","model.fit_generator(train_generator_fn(),\n","                    validation_data=val_generator_fn(),\n","                    steps_per_epoch=n_train_samples,\n","                    validation_steps=validation_steps,\n","                    epochs=epochs,\n","                    callbacks=[model_checkpoint])"]},{"cell_type":"markdown","metadata":{"id":"fny7K-Q3hd6s"},"source":["## Let's test the model on sample images"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121834,"status":"aborted","timestamp":1749114210075,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"vTP89OeAECaz"},"outputs":[],"source":["visualize_seg(next(val_image_generator)[0], next(val_mask_generator)[0], gt_mode='sparse')"]},{"cell_type":"markdown","metadata":{"id":"vrd3C7fpFuAy"},"source":["## Data augmentation\n","\n","When doing data augmentation we have to take of few things:\n","\n","1- No transform to change the pixel values of the masks. Otherwise, the colors ranges will not be found in the dict.\n","\n","2- Any geometric transform must be applied equally on the mask and the img\n","\n","3- Fix the seed between mask and img generators, so that the same operation is applied to both.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121837,"status":"aborted","timestamp":1749114210079,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"T99C1ZbYFwH1"},"outputs":[],"source":["# Data generator\n","#batch_sz = 4\n","#https://keras.io/preprocessing/image/\n","from keras.preprocessing.image import ImageDataGenerator\n","# we create two instances with the same arguments\n","\n","# VI Note: use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n","data_gen_args = dict(rotation_range=0.2,\n","                    width_shift_range=0.05,\n","                    height_shift_range=0.05,\n","                    shear_range=0.05,\n","                    zoom_range=0.05,\n","                    horizontal_flip=True,\n","                    fill_mode='nearest',\n","                    rescale=1./255)\n","\n","mask_gen_args = dict(rotation_range=0.2,\n","                    width_shift_range=0.05,\n","                    height_shift_range=0.05,\n","                    shear_range=0.05,\n","                    zoom_range=0.05,\n","                    horizontal_flip=True,\n","                    fill_mode='nearest')\n","\n","\n","image_datagen = ImageDataGenerator(**data_gen_args)\n","mask_datagen  = ImageDataGenerator(**mask_gen_args)\n","\n","# Provide the same seed and keyword arguments to the fit and flow methods\n","seed = 1\n","#image_datagen.fit(images, augment=True, seed=seed)\n","#mask_datagen.fit(masks, augment=True, seed=seed)\n","\n","image_generator = image_datagen.flow_from_directory(\n","    data_path,\n","    class_mode=None,\n","    classes=['train'],\n","    seed=seed,\n","    batch_size=batch_sz,\n","    target_size=(256,256))\n","\n","mask_generator = mask_datagen.flow_from_directory(\n","    data_path,\n","    classes=['train_labels'],\n","    class_mode=None,\n","    seed=seed,\n","    batch_size=batch_sz,\n","    color_mode='rgb',\n","    target_size=(256,256))\n","\n","# combine generators into one which yields image and masks\n","train_generator = zip(image_generator, mask_generator)\n","\n","\n","val_image_generator = image_datagen.flow_from_directory(\n","    data_path,\n","    class_mode=None,\n","    classes=['val'],\n","    seed=seed,\n","    batch_size=batch_sz,\n","    target_size=(256,256))\n","\n","val_mask_generator = mask_datagen.flow_from_directory(\n","    data_path,\n","    classes=['val_labels'],\n","    class_mode=None,\n","    seed=seed,\n","    batch_size=batch_sz,\n","    color_mode='rgb',\n","    target_size=(256,256))\n","\n","# combine generators into one which yields image and masks\n","val_generator = zip(val_image_generator, val_mask_generator)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121845,"status":"aborted","timestamp":1749114210088,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"LH8MdZpljB3G"},"outputs":[],"source":["def val_generator_fn():\n","\n","    for (img,mask) in val_generator:\n","        new_mask = adjust_mask(mask)\n","        yield (img,new_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121849,"status":"aborted","timestamp":1749114210092,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"XrJk8xA1jhZJ"},"outputs":[],"source":["\n","def train_generator_fn():\n","    for (img,mask) in train_generator:\n","        new_mask = adjust_mask(mask)\n","        yield (img,new_mask)"]},{"cell_type":"markdown","metadata":{"id":"mLZ9GxiCWSx7"},"source":["\n","\n","Let's take a look on some augmented images and masks"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121852,"status":"aborted","timestamp":1749114210096,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"uhtcWPaOWl2C"},"outputs":[],"source":["img = load_img(str(data_path) + '/train/0001TP_006690.png', target_size=(256,256))\n","img"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121856,"status":"aborted","timestamp":1749114210102,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"KjWsjYNuRZCN"},"outputs":[],"source":["mask = load_img(str(data_path) + '/train_labels/0001TP_006690_L.png', target_size=(256,256))\n","mask"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121860,"status":"aborted","timestamp":1749114210106,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"IMTa74oJWrFQ"},"outputs":[],"source":["# The .flow() command below generates batches of randomly transformed images.\n","# It will loop indefinitely, so we need to `break` the loop at some point!\n","from keras.preprocessing.image import array_to_img, img_to_array\n","i = 0\n","img = img_to_array(img)\n","mask = img_to_array(mask)\n","for aug_img, aug_mask in zip(image_datagen.flow(np.expand_dims(img, 0), batch_size=1), mask_datagen.flow(np.expand_dims(mask, 0), batch_size=1)):\n","    plt.figure(i)\n","    plt.subplot(221)\n","    imgplot = plt.imshow(array_to_img(aug_img[0]))\n","    plt.subplot(222)\n","    imgplot = plt.imshow(array_to_img(aug_mask[0]))\n","    i += 1\n","    if i > 10:\n","        break\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121857,"status":"aborted","timestamp":1749114210110,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"YykK7jXderjV"},"outputs":[],"source":["model = unet(n_classes)\n"]},{"cell_type":"markdown","metadata":{"id":"MhGdMDEhN2eJ"},"source":["Since we are doing data augmentation, we don't have a static data set, so we use the steps_per_epoch to control how many augmented samples per epoch we want. Let's have 1000."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121858,"status":"aborted","timestamp":1749114210117,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"eIHI3G6Heyup"},"outputs":[],"source":["model_checkpoint = ModelCheckpoint('unet_camvid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n","model.fit_generator(train_generator_fn(),\n","                    validation_data=val_generator_fn(),\n","                    steps_per_epoch=steps_per_epoch,\n","                    validation_steps=validation_steps,\n","                    epochs=epochs,\n","                    callbacks=[model_checkpoint])"]},{"cell_type":"markdown","metadata":{"id":"AbrkeENYfbGV"},"source":["# Let's try some samples"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121855,"status":"aborted","timestamp":1749114210121,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"EwMaeapYfajC"},"outputs":[],"source":["img = next(val_image_generator)[0]\n","gt_img = next(val_mask_generator)[0]\n","visualize_seg(img, gt_img, gt_mode='sparse')\n"]},{"cell_type":"markdown","metadata":{"id":"5PpIbhBqb1GB"},"source":["# Data augmentation on data in RAM\n","\n","In the above we link data augmentation to data loading.\n","\n","In fact they are both independent.\n","\n","Data augmentation is defined in the `ImageDataGenerator` class, or any other generator using yield\n","\n","The generator can be used with files on disk (data loader), if we use flow_from_directory function.\n","\n","It can also be used with data in RAM, using just the flow function"]},{"cell_type":"markdown","metadata":{"id":"m8v46yzY2XDY"},"source":["Since we are calling adjust_mask inside the generator, so we need to load the raw mask, and leave the generator to do its job.\n","\n","We need to modify load_CAMVID NOT to load OHE masks, but raw colors:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTXEcsjF2j1m","executionInfo":{"status":"aborted","timestamp":1749114210126,"user_tz":-180,"elapsed":121859,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["def load_raw_CAMVID(data_type='train', enc='ohe', shape='normal'):\n","  img_path = str(data_path) + '/' + data_type + '/'\n","  labels_path = str(data_path) + '/' + data_type + '_labels/'\n","  # without adding target_size=(256,256) in load_img we get Out of mem: 421x960x720x32x4bytes is around 34GB!\n","  x = np.array([np.array(load_img(str(img_path) + file, target_size=(256,256)))*1./255 for file in sorted(os.listdir(img_path))])\n","  if(enc=='ohe'):\n","\n","    y = np.array([np.array(load_img(str(labels_path) + file, target_size=(256,256))) for file in sorted(os.listdir(labels_path))])\n","  elif(enc=='sparse_cat'):\n","    y = np.array([np.array(load_img(str(labels_path) + file, target_size=(256,256))) for file in sorted(os.listdir(labels_path))])\n","  if(shape == 'flat'):\n","    y = np.reshape(y.shape[0], y.shape[1]*y.shape[2])\n","    y = np.expand_dims(y, axis=-1)\n","  return x, y"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121858,"status":"aborted","timestamp":1749114210131,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"Ew_gPuyJ262f"},"outputs":[],"source":["import time\n","start = time.time()\n","x_train, y_train = load_raw_CAMVID(data_type='train')\n","#x_test, y_test = load_raw_CAMVID(data_type='test')# Don't load test for RAM consumption\n","x_val, y_val = load_raw_CAMVID(data_type='val')\n","end = time.time()\n","print('Time elapsed: ', end-start)\n","\n","print(x_train.shape)\n","print(y_train.shape)\n","#print(x_test.shape)\n","#print(y_test.shape)\n","print(x_val.shape)\n","print(y_val.shape)"]},{"cell_type":"markdown","metadata":{"id":"qbE8gHV2O-Ac"},"source":["__Since we scaled while loading the data, we don't need to scale in the generator__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsuZTde13aZJ","executionInfo":{"status":"aborted","timestamp":1749114210136,"user_tz":-180,"elapsed":121862,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"}}},"outputs":[],"source":["# Data generator\n","#batch_sz = 4\n","#https://keras.io/preprocessing/image/\n","from keras.preprocessing.image import ImageDataGenerator\n","# we create two instances with the same arguments\n","\n","# VI Note: use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n","data_gen_args = dict(rotation_range=0.2,\n","                    width_shift_range=0.05,\n","                    height_shift_range=0.05,\n","                    shear_range=0.05,\n","                    zoom_range=0.05,\n","                    horizontal_flip=True,\n","                    fill_mode='nearest')\n","                    #rescale=1./255)# Data is already scaled when loaded\n","\n","mask_gen_args = dict(rotation_range=0.2,\n","                    width_shift_range=0.05,\n","                    height_shift_range=0.05,\n","                    shear_range=0.05,\n","                    zoom_range=0.05,\n","                    horizontal_flip=True,\n","                    fill_mode='nearest')\n","                    #preprocessing_function=adjust_mask)# This is not possible since the preprocessing_function can only return the same shape as image\n","\n","image_datagen = ImageDataGenerator(**data_gen_args)\n","mask_datagen  = ImageDataGenerator(**mask_gen_args)\n","\n","# Provide the same seed and keyword arguments to the fit and flow methods\n","seed = 1\n","#image_datagen.fit(images, augment=True, seed=seed)\n","#mask_datagen.fit(masks, augment=True, seed=seed)\n","\n","image_generator = image_datagen.flow(\n","    x_train,\n","    seed=seed,\n","    batch_size=batch_sz)\n","\n","mask_generator = mask_datagen.flow(\n","    y_train,\n","    seed=seed,\n","    batch_size=batch_sz)\n","\n","# combine generators into one which yields image and masks\n","train_generator = zip(image_generator, mask_generator)\n","\n","def train_generator_fn():\n","\n","    for (img,mask) in train_generator:\n","        new_mask = adjust_mask(mask)\n","        yield (img,new_mask)\n","\n","val_image_generator = image_datagen.flow(\n","    x_val,\n","    seed=seed,\n","    batch_size=batch_sz)\n","\n","val_mask_generator = mask_datagen.flow(\n","    y_val,\n","    seed=seed,\n","    batch_size=batch_sz)\n","\n","# combine generators into one which yields image and masks\n","val_generator = zip(val_image_generator, val_mask_generator)\n","\n","def val_generator_fn():\n","\n","    for (img,mask) in val_generator:\n","        new_mask = adjust_mask(mask)\n","        yield (img,new_mask)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121861,"status":"aborted","timestamp":1749114210142,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"yVwtajJo3B5E"},"outputs":[],"source":["model = unet(n_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121859,"status":"aborted","timestamp":1749114210146,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"L8rnk13o3Cap"},"outputs":[],"source":["model_checkpoint = ModelCheckpoint('unet_camvid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n","model.fit_generator(train_generator_fn(),\n","                    validation_data=val_generator_fn(),\n","                    steps_per_epoch=steps_per_epoch,\n","                    validation_steps=validation_steps,\n","                    epochs=epochs,\n","                    callbacks=[model_checkpoint])"]},{"cell_type":"markdown","metadata":{"id":"WPT2kZujg6cn"},"source":["# Try some samples"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":121861,"status":"aborted","timestamp":1749114210154,"user":{"displayName":"Abdulrahman gamal","userId":"04146689833803247836"},"user_tz":-180},"id":"D6mOqZT2e98w"},"outputs":[],"source":["img = next(val_image_generator)[0]\n","gt_img = next(val_mask_generator)[0]\n","visualize_seg(img, gt_img, gt_mode='sparse')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"13ID9lzDmSTfQgIuGwME6NXBwjZyMKrUd","timestamp":1748683585007},{"file_id":"1B2zOcx2BmL2oJZHKIwPOzgjm4t-WQ-ug","timestamp":1565178174121},{"file_id":"1xZucj0-kElSN8OTydsZj_AelSrwVhr0n","timestamp":1564395595668}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":0}